{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqUO42Nyl8iY"
   },
   "source": [
    "Preparing colab to import kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "collapsed": true,
    "id": "yFbqoTYrlx-V",
    "outputId": "bd4702fd-1265-4625-8737-e443b49d0803",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "\n",
    "from google.colab import files\n",
    "_ = files.upload()\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmnkTRj7l7Tl"
   },
   "source": [
    "\n",
    "download and unzip dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kyrarj3YmIms",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d adityajn105/flickr8k\n",
    "!unzip flickr8k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGnz5su6mSVc"
   },
   "source": [
    "Importing used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Y0YoaTLGmPTe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import nltk\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import torchvision.models as models # import resnet18, ResNet18_Weights\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as T\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjhC6MXNmZpw"
   },
   "source": [
    "Importing used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Shcd8-BJmYvJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "START_WORD  = '<SOS>'\n",
    "END_WORD    = '<EOS>'\n",
    "PADDING_WORD= '<PAD>'\n",
    "UNKNOWN_WORD= '<UNK>'\n",
    "GUID_TOKENS = [START_WORD, END_WORD, PADDING_WORD, UNKNOWN_WORD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "mcWiUIcEmdle",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMAGES_PATH = './Images/'\n",
    "ANNOTATION_FILE = './captions.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU2uLQELmhPp"
   },
   "source": [
    "Vocabulary maps words to index and indexes to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "95wpAvbUmfZr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Vocabulary():\n",
    "  def __init__(self):\n",
    "    self.idx2token = {0: START_WORD, 1: END_WORD, 2: PADDING_WORD, 3: UNKNOWN_WORD}\n",
    "    self.token2idx = {v:k for k,v in self.idx2token.items()}\n",
    "    self.max_cap_len = 0\n",
    "  def __len__(self):\n",
    "    return len(self.idx2token)\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    tokens = list(filter(len, text.translate(str.maketrans('', '', string.punctuation+'\\n')).lower().split()))\n",
    "    if len(tokens) > self.max_cap_len:\n",
    "      self.max_cap_len = len(tokens)\n",
    "    return tokens\n",
    "\n",
    "  def fill_vocab(self, sentence_list):\n",
    "    idx = max(list(self.idx2token.keys()))\n",
    "    for sentence in sentence_list:\n",
    "      for word in self.tokenize(sentence):\n",
    "        if word not in self.token2idx:\n",
    "          idx += 1\n",
    "          self.token2idx[word] = idx\n",
    "          self.idx2token[idx] = word\n",
    "\n",
    "  def numericalize(self, text):\n",
    "    tokenized_text = self.tokenize(text)\n",
    "    return [ self.token2idx[token] if token in self.token2idx else self.token2idx[UNKNOWN_WORD] for token in tokenized_text ]\n",
    "\n",
    "  def stringify(self, idxVec):\n",
    "    return filter(lambda word: word not in [START_WORD, END_WORD, PADDING_WORD], [ self.idx2token[idx] if idx in self.idx2token else UNKNOWN_WORD for idx in idxVec.tolist() ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxXkkigkmphO"
   },
   "source": [
    "Dataset gets images file and tokenizes caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "HcUSraWAmr0k",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Flickr8KDataset(utils.data.Dataset):\n",
    "  def __init__(self, images_dir, captions_file, transform=None, mode='train', train_test_split=0.1):\n",
    "    self.images_dir = images_dir\n",
    "    self.df = pd.read_csv(captions_file)\n",
    "    self.transform = transform\n",
    "    self.images = self.df[\"image\"]\n",
    "    self.captions = self.df[\"caption\"]\n",
    "    self.vocab = Vocabulary()\n",
    "    self.vocab.fill_vocab(self.captions.tolist())\n",
    "    self.mode = mode\n",
    "    self.train_test_split = train_test_split\n",
    "\n",
    "  def __len__(self):\n",
    "    return int(len(self.df) * (1 - self.train_test_split)) if self.mode == 'train' else int(len(self.df) * self.train_test_split)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    if self.mode != 'train':\n",
    "      idx = len(self) - idx\n",
    "    caption = self.captions[idx]\n",
    "    img_name = self.images[idx]\n",
    "    img_location = os.path.join(self.images_dir,img_name)\n",
    "    img = Image.open(img_location).convert(\"RGB\")\n",
    "\n",
    "    if self.transform is not None:\n",
    "      img = self.transform(img)\n",
    "\n",
    "    caption_vec = []\n",
    "    caption_vec += [self.vocab.token2idx[START_WORD]]\n",
    "    caption_vec += self.vocab.numericalize(caption)\n",
    "    caption_vec += [self.vocab.token2idx[END_WORD]]\n",
    "\n",
    "    return img, torch.tensor(caption_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JK0OqkhAmwSx"
   },
   "source": [
    "Collate is used for padding in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "23Dhdh5jmxCD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CapsCollate():\n",
    "  def __init__(self, pad_idx, batch_first=False):\n",
    "    self.pad_idx = pad_idx\n",
    "    self.batch_first = batch_first\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "    imgs = torch.cat(imgs,dim=0)\n",
    "    targets = [item[1] for item in batch]\n",
    "    targets = pad_sequence(targets, batch_first=self.batch_first, padding_value = self.pad_idx)\n",
    "    return imgs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQRozfBdmzhI"
   },
   "source": [
    "Defining constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "mL6W4v8dm1-C",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_WORKER = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adkLhdVlm48H"
   },
   "source": [
    "Common transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pAudEjLVm4YX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.Resize(226),\n",
    "    T.RandomCrop(224),\n",
    "    T.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcyzSPnLm-xU"
   },
   "source": [
    "Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "vKPTzWQ5nBVz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset =  Flickr8KDataset(\n",
    "    images_dir = IMAGES_PATH,\n",
    "    captions_file = ANNOTATION_FILE,\n",
    "    transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsUPkVMXnInu"
   },
   "source": [
    "Defining device type and vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DP6ThxpVnCs1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(dataset.vocab)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "max_cap_len = dataset.vocab.max_cap_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhHzcU-CnO3T"
   },
   "source": [
    "Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9M7hcj91nQfJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader = utils.data.DataLoader(\n",
    "    dataset = dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = NUM_WORKER,\n",
    "    shuffle = True,\n",
    "    collate_fn = CapsCollate(pad_idx=dataset.vocab.token2idx[PADDING_WORD], batch_first=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f86iC7p5nSMQ"
   },
   "source": [
    "a function to show image with its caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "OUiqXZT6nTl3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_image(inp, title=None):\n",
    "  inp = inp.numpy().transpose((1,2,0))\n",
    "  plt.imshow(inp)\n",
    "  if title is not None:\n",
    "    toks = dataset.vocab.stringify(title)\n",
    "    caption = ' '.join(toks)\n",
    "    plt.title(caption)\n",
    "  plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "img, caps = dataset[0]\n",
    "show_image(img,caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVbaUKiAnhlU"
   },
   "source": [
    "Preview of resnet18 to understand architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QwBOR4SLnU8-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checking_resnet = models.resnet18(pretrained=True)\n",
    "print(checking_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SpgkXtPnmdd"
   },
   "source": [
    "CNN part of model\n",
    "- uses resnet18 except last layer (fc)\n",
    "- uses a linear layer to embed features to input size of LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5r2S_CBknpfI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, embed_size, freeze=True):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        if freeze:\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad_(False)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.embed.weight.data.normal_(0., 0.02)\n",
    "        self.embed.bias.data.fill_(0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.batch = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = F.relu(self.batch(self.embed(features)))\n",
    "        features = self.dropout(features)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsKr6Mucntl8"
   },
   "source": [
    "RNN part\n",
    "- uses trainable embeding layer\n",
    "- uses LSTM as RNN to create captions\n",
    "- uses a linear layer to translate hiddens to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fKVkEUI3nt8U",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=300, hidden_size=256, num_layers=1, dropout=0.5):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.vocabulary_size = vocab_size\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocabulary_size, self.embed_size)\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  # Adding dropout layer with specified dropout probability\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, self.vocabulary_size)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = self.dropout(embeddings)  # Applying dropout after the embedding layer\n",
    "        features = features.unsqueeze(1)\n",
    "        embeddings = torch.cat((features, embeddings[:, :-1, :]), dim=1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crgP31FIn0dl"
   },
   "source": [
    "Merging two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "YsA0UJQTnwBX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ImgCap_Model(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size = 300, hidden_size = 256, freeze = True):\n",
    "    super(ImgCap_Model, self).__init__()\n",
    "    self.cnn = CNN_Model(embed_size = embed_size, freeze = freeze)\n",
    "    self.lstm = LSTM_Model(vocab_size = vocab_size, embed_size = embed_size, hidden_size = hidden_size)\n",
    "\n",
    "  def forward(self, images, captions):\n",
    "    features = self.cnn(images)\n",
    "    outputs = self.lstm(features, captions)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQFzAtt9n4IT"
   },
   "source": [
    "Sample model 1\n",
    "- all layers except last linear layer are freezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "AZ_bqQMWnyGq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_freezed = ImgCap_Model(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "PYnv_CYPn7Nr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "learning_rate = 3e-4\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.token2idx[PADDING_WORD])\n",
    "optimizer = optim.Adam(model_freezed.parameters(), lr=learning_rate)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOyWUfs6oA3h"
   },
   "source": [
    "defining function of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "qThoZrWJn-b8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader_test = utils.data.DataLoader(\n",
    "    dataset = dataset,\n",
    "    batch_size = 1,\n",
    "    num_workers = NUM_WORKER,\n",
    "    shuffle = True,\n",
    "    collate_fn = CapsCollate(pad_idx=dataset.vocab.token2idx[PADDING_WORD], batch_first=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0DZ74xghoDLO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, criterion, device):\n",
    "  model.eval()\n",
    "  data_loader.dataset.mode = 'test'\n",
    "  loss = 0.0\n",
    "  for idx ,(image, caption) in enumerate(iter(data_loader)):\n",
    "    image = image.to(device)\n",
    "    caption = caption.to(device)\n",
    "    raw_cap = [dataset.vocab.token2idx[PADDING_WORD] for _ in range(caption.size(dim=1))]\n",
    "    raw_cap = torch.tensor([raw_cap])\n",
    "    raw_cap = raw_cap.to(device)\n",
    "    result = None\n",
    "    for i in range(len(caption)):\n",
    "      result = model(image, raw_cap)\n",
    "      result_t = result.contiguous().view(-1, vocab_size)\n",
    "      raw_cap = result_t\n",
    "      raw_cap = raw_cap.to(device)\n",
    "    loss_val = criterion(result.contiguous().view(-1, vocab_size), caption.view(-1))\n",
    "    loss += loss_val.item()\n",
    "  return loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "M3m2dA0ioFQ7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs, optimizer, criterion, data_loader, val_data_loader, device):\n",
    "  steps = len(val_data_loader.dataset) / BATCH_SIZE\n",
    "  history_loss = []\n",
    "  history_loss_val = []\n",
    "  oldtime = time.time()\n",
    "  loss = 0\n",
    "  for epoch in range(1, epochs + 1):\n",
    "    oldtime = time.time()\n",
    "    data_loader.dataset.mode = 'train'\n",
    "    model.train()\n",
    "    for idx, (image, captions) in enumerate(iter(data_loader)):\n",
    "      image = image.to(device)\n",
    "      captions = captions.to(device)\n",
    "\n",
    "      model.zero_grad()\n",
    "\n",
    "      outputs = model(image, captions)\n",
    "\n",
    "      loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "      stats = 'Epoch [%d/%d], Step [%d/%d], loss: %.4f' % (epoch, epochs, idx, steps, loss.item())\n",
    "      print('\\r' + stats, end=\"\")\n",
    "    train_time = time.time()\n",
    "    val_loss = eval_model(model, val_data_loader, criterion, device)\n",
    "    stats = ', val_loss: %.4f, %.3f s' % (val_loss, train_time - oldtime)\n",
    "    print(stats)\n",
    "    history_loss.append(loss.item())\n",
    "    history_loss_val.append(val_loss)\n",
    "  return history_loss, history_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "k96gQEVpoGyo",
    "outputId": "29d93fea-fb68-49f1-deef-537cda0a007d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_freezed.to(device)\n",
    "loss_history, history_loss_val = train_model(model_freezed, EPOCHS, optimizer, criterion, data_loader, data_loader_test, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iD08nBfoK2r"
   },
   "source": [
    "error plot of model 1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "TQxILeYDoLP2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([loss for loss in loss_history], color='crimson', linewidth=2, label='train loss')\n",
    "plt.plot([loss for loss in history_loss_val], color='blue', linewidth=2, label='validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "X5-jvPQSoMmt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_freezed.eval()\n",
    "for idx, (images, captions) in enumerate(iter(data_loader_test)):\n",
    "  if idx % 5 != 2:\n",
    "    continue\n",
    "  images = images.to(device)\n",
    "  captions = captions.to(device)\n",
    "  raw_cap = [dataset.vocab.token2idx[PADDING_WORD] for _ in range(max_cap_len)]\n",
    "  raw_cap = torch.tensor([raw_cap])\n",
    "  raw_cap = raw_cap.to(device)\n",
    "  result = None\n",
    "  for i in range(max_cap_len):\n",
    "    result = model_freezed(images, raw_cap)\n",
    "    result = [i.argmax() for i in result.cpu().data.numpy()[0]]\n",
    "    raw_cap = torch.tensor([result])\n",
    "    raw_cap = raw_cap.to(device)\n",
    "  show_image(images.cpu().data[0], raw_cap[0])\n",
    "  if idx >= 20:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEOmb0hyoSrh"
   },
   "source": [
    "sample model 2\n",
    "- all layers are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "wg_8D-ayoPRs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_unfreezed = ImgCap_Model(vocab_size, freeze = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlQOQKmXoZMD"
   },
   "source": [
    "Create another dataloader with same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kuGPFSe1oWMe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "v_sjkkfnoeVJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader_un = utils.data.DataLoader(\n",
    "    dataset = dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = NUM_WORKER,\n",
    "    shuffle = True,\n",
    "    collate_fn = CapsCollate(pad_idx=dataset.vocab.token2idx[PADDING_WORD], batch_first=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "zDwt_lSIoflH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader_test_un = utils.data.DataLoader(\n",
    "    dataset = dataset,\n",
    "    batch_size = 1,\n",
    "    num_workers = NUM_WORKER,\n",
    "    shuffle = True,\n",
    "    collate_fn = CapsCollate(pad_idx=dataset.vocab.token2idx[PADDING_WORD], batch_first=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "OA3lDUuQohvs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion_un = nn.CrossEntropyLoss(ignore_index=dataset.vocab.token2idx[PADDING_WORD])\n",
    "optimizer_un = optim.Adam(model_unfreezed.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXwjV_OBolL6"
   },
   "source": [
    "Training model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RPlQI8AOoi_i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_unfreezed.to(device)\n",
    "loss_history_un, val_loss_history_un = train_model(model_unfreezed, EPOCHS, optimizer_un, criterion_un, data_loader_un, data_loader_test_un, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "su1M8G8hoopr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([loss for loss in loss_history_un], color='crimson', linewidth=2, label='train loss')\n",
    "plt.plot([loss for loss in val_loss_history_un], color='blue', linewidth=2, label='validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JQWUX8AaorQT",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_unfreezed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_unfreezed\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (images, captions) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_loader_test_un)):\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_unfreezed' is not defined"
     ]
    }
   ],
   "source": [
    "model_unfreezed.eval()\n",
    "for idx, (images, captions) in enumerate(iter(data_loader_test_un)):\n",
    "  if idx % 5 != 2:\n",
    "    continue\n",
    "  images = images.to(device)\n",
    "  captions = captions.to(device)\n",
    "  raw_cap = [dataset.vocab.token2idx[PADDING_WORD] for _ in range(max_cap_len)]\n",
    "  raw_cap = torch.tensor([raw_cap])\n",
    "  raw_cap = raw_cap.to(device)\n",
    "  result = None\n",
    "  for i in range(max_cap_len):\n",
    "    result = model_unfreezed(images, raw_cap)\n",
    "    result = [i.argmax() for i in result.cpu().data.numpy()[0]]\n",
    "    raw_cap = torch.tensor([result])\n",
    "    raw_cap = raw_cap.to(device)\n",
    "  show_image(images.cpu().data[0], raw_cap[0])\n",
    "  if idx >= 20:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MMnNbmedotT5",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
